#!/usr/bin/env perl

=pod

=head1 NAME

uniproc - Universal data processing tool

=head1 SYNOPSIS

uniproc [I<OPTIONS>] I<INPUTFILE> I<COMMAND> [I<ARGS>]

=head1 DESCRIPTION

Take each line from I<INPUTFILE> as B<DATA> (chopping end-of-line chars),
pass each TAB-delimited fields of B<DATA> to I<COMMAND> as arguments after I<ARGS>
(unless placeholder is in I<COMMAND> or I<ARGS>, see below),
run I<COMMAND> and then record the exit status.

Can be parallelized well.
uniproc(1) itself does not run multiple instances of I<COMMAND> in parallel, just in series,
but if you start multiple instances of uniproc(1), then you can run I<COMMAND>s concurrently.
Locking ensures no overlapping data being processed.
So you don't need special precautions (locking, data partitioning) when starting uniproc(1) multiple times on the same I<INPUTFILE>.

Use a wrapper command/script for I<COMMAND> if you want either of these:

=over 4

=item save I<COMMAND>'s output as well.

By default it goes to STDOUT.
Use redirexec(1) for example.

=item pass B<DATA> on the STDIN or in environment variable instead of command argument.

Use args2env(1) or args2stdin(1) for example.

=back

If re-run after an interrupt, won't process already processed data.
But you may re-try the failed ones by the B<--retry> option.

The user is allowed to append new lines of data to I<INPUTFILE> between executions or during runtime - it won't mess up the processing.
However editing or reordering lines which are already in the file, confuses the results - don't do it.

I<ARGS> (and I<COMMAND> too, somewhat usefully) supports placeholders:
A curly bracket-pair C<{}> is replaced to B<DATA> as one argument, including TAB chars if any, anywhere in I<COMMAND> I<ARGS>.
If there is a number in it, B<{>I<N>B<}>, then the I<N>th TAB-delimited field (1-indexed) is gonna be substituted in.
A lone B<{@}> argument expands to as many arguments as TAB-delimited fields there are in B<DATA>.
Multiple numbers in the placeholder like B<{5,3,4}> expands to all of the data fields specified by the index numbers, into multiple arguments.
Note that in this case, the multi-index placeholder must stand in its own separate argument, just as the all-fields B<{@}> placeholder.
Indexing a non-existing field expands to empty string.
Be aware that your shell (eg. bash(1)) may expand arguments like C<{5,3,4}> before it gets to uniproc(1),
so escape it if neccessary (eg. C<'{5,3,4}'>).
If there is any curly bracket placeholder like these, B<DATA> fields won't be added to I<ARGS> as the last argument.

=head1 OPTIONS

=over 4

=item -r, --retry

Process those data which were earlier failed (according to B<< I<INPUTFILE>.uniproc >> state file) too,
besides the unprocessed ones.

=item -f, --failed

Process only the earlier failed items.

=item -1, --one-item

Process only 1 item, then exit.
Default is to process as many items in series as possible.

=item -n, --items I<NUM>

How many items to process.

=item -e, --errexit

Stop processing items as soon as the first I<COMMAND> exits non-zero,
and uniproc(1) itself exists which that exit code (or 128+signal if signaled).

=item -sp, --show-progress

Show which item is being started to process.

=item -sd, --show-data

Show the raw data what is being started to process.

=item -ss, --show-summary

Show stats summary when exit.

=back

=head1 FILES

It maintains I<INPUTFILE>.uniproc file
by writing the processing status of each lines of input data in it line-by-line.
Processing status is either:

=over 4

=item all spaces (C<   >)

processing not yet started

=item periods (C<...>)

in progress

=item digits, possibly padded by spaces (C<  0>)

result status (exit code)

=item exclamation mark (C<!>) followed by hexadecimal digits (C<!0f>)

termination signal (I<COMMAND> teminated abnormally)

=item EOF (ie. fewer lines than input data)

processing of this item has not started yet

=back

I<INPUTFILE>.uniproc is locked while read/write to ensure consistency.
I<INPUTFILE>.uniproc.B<NUM> are the name of the files which hold the lock for the currently in-progress processes,
where B<NUM> is the line number of the corresponding piece of data in I<INPUTFILE>.
A lock is held on each of these I<INPUTFILE>.proc.B<NUM> files by the respective instance of I<COMMAND>
to detect if the processing is still going or the process crashed.

=head1 LIMITATION

Due to currently used locking mechanism (Fcntl(3perl)), running on multiple hosts may disrespect locking.

=head1 ENVIRONMENT

When running I<COMMAND>, the following environment is set:

=over 4

=item UNIPROC_DATANUM

Number of the particular piece of data (ie. line number in I<INPUTFILE>)
which is need to be processed by the current process.

=item UNIPROC_DATANUM_1INDEX

Same as B<UNIPROC_DATANUM> but 1-indexed instead of 0-indexed.

=item UNIPROC_TOTALNUM

Total number of items (processed and unprocessed).

=back

=head1 EXAMPLES

Display the data processing status before each line of data:

  paste datafile.uniproc datafile

How much competed?

  awk -v total=$(wc -l < datafile) 'BEGIN{ok=ip=fail=0} {if($1==0){ok++} else if($1=="..."){ip++} else if($1!=""){fail++}} END{print "total: "total", completed: "ok" ("(ok*100/total)"%), in-progress: "ip" ("(ip*100/total)"%), failed: "fail" ("(fail*100/total)"%)"}' datafile.uniproc
  
Output:

  total: 8, completed: 4 (50%), in-progress: 1 (12.5%), failed: 1 (12.5%)

Record output of data processing into a file per each data item:

  uniproc datafile sh -c 'some-command "$@" | tee output-$UNIPROC_DATANUM' --

  uniproc datafile substenv -e UNIPROC_DATANUM redirexec '1:a:file:output-$UNIPROC_DATANUM' some-command

Same as above, plus keep the output on STDOUT as well as in separate files.
Note, the C<{}> argument is there to pass B<DATA> to the right command:

  uniproc datafile pipecmd some-command {} -- substenv -e UNIPROC_DATANUM tee -a 'output-$UNIPROC_DATANUM'

Display data number, processing status, input data, (last line of) output data in a table:

  join -t $'\t' <(nl -ba -v0 datafile.uniproc) <(nl -ba -v0 datafile) | foreach -t --prefix-add-data --prefix-add-tab tail -n1 output-{0}

=cut


use Fcntl qw/:flock :seek/;
use Data::Dumper;
use Getopt::Long qw/:config no_ignore_case no_bundling no_getopt_compat no_auto_abbrev require_order/;
use IPC::Run qw/run/;
use POSIX;
use Pod::Usage;

$OptProcessNew = 1;
$OptProcessFailed = 0;
$OptShots = undef;
$OptShowProgress = 0;
$OptShowData = 0;
$OptShowSummary = 0;
$OptDebug = 0;
$OptErrExit = 0;

GetOptions(
	'r|retry' => sub { $OptProcessFailed = 1; $OptProcessNew = 1; },
	'f|failed' => sub { $OptProcessFailed = 1; $OptProcessNew = 0; },
	'1|oneshot|one-item' => sub { $OptShots = 1; },
	'n|items=i' => \$OptShots,
	'sp|show-progress' => \$OptShowProgress,
	'sd|show-data' => \$OptShowData,
	'ss|show-summary' => \$OptShowSummary,
	'e|errexit!' => \$OptErrExit,
	'debug!' => \$OptDebug,
	'help' => sub { pod2usage(-exitval=>0, -verbose=>99); },
) or pod2usage(-exitval=>2, -verbose=>99);

die "$0: missing command\n" unless scalar @ARGV >= 2;


sub debug_msg
{
	warn @_ if $OptDebug;
}

sub deriv_processing_status
{
	my $child_status = shift;
	my $status = sprintf '%3d', WEXITSTATUS($child_status);
	$status = sprintf '!%02x', WTERMSIG($child_status) if WIFSIGNALED($child_status);
	return $status;
}

sub fopen
{
	my $path = shift;
	my $opts = shift;  # supported opts: rw, no_create, lock
	my $mode = '<';  # default mode is read-only, no-create
	if($opts->{'rw'})
	{
		if(not $opts->{'no_create'})
		{
			open my $fh, '>>', $path or die "$0: $path: $!\n";
			close $fh;
		}
		$mode = '+<';
	}
	
	open my $fh, $mode, $path or die "$0: $path: $!\n";
	seek $fh, 0, SEEK_SET or die "$0: seek: $path: $!\n";
	if($opts->{'lock'})
	{
		flock $fh, LOCK_EX or die "$0: flock: $path: $!\n";
	}
	return $fh;
}

sub extend_resultsfile
{
	my $fname = $ResultsFile;
	my $fh = shift;
	my $extended_size = shift;
	
	seek $fh, 0, SEEK_END;
	my $size = tell $fh;
	my $endpos_last_complete_record = int($size / $ResultsRecordLength) * $ResultsRecordLength;
	my $records_to_append = ($extended_size - $endpos_last_complete_record) / $ResultsRecordLength;
	debug_msg "go to offset $endpos_last_complete_record to extend by $records_to_append records\n";
	seek $fh, $size, SEEK_SET;
	# fill up with empty status records
	print {$Results_fh} "   \n" x $records_to_append or die "$0: write: $fname: $!\n";
}

sub record_status
{
	my $linenum = shift;
	my $status = shift;
	die "$0: size mismatch: length(\"$status\") != $ResultsRecordLength - 1\n" if length($status) != $ResultsRecordLength - 1;
	
	my $offset = $linenum * $ResultsRecordLength;
	debug_msg "go to offset $offset to record data # $linenum 's status \"$status\"\n";
	seek $Results_fh, $offset, SEEK_SET;
	if(eof $Results_fh)
	{
		debug_msg "eof\n";
		# results file is not big enough, let's extend
		extend_resultsfile($Results_fh, $offset);
		seek $Results_fh, $offset, SEEK_SET;
	}
	print {$Results_fh} "$status\n" or die "$0: write: $ResultsFile: $!\n";
}

sub input_data
{
	# TODO use index file, if exists, to seek in.
	
	my $asked_line = shift;
	my $fh = fopen $InputFile, {no_create=>1,};
	my $linenum = 0;
	my $data = undef;
	while(my $line = <$fh>)
	{
		if($linenum == $asked_line)
		{
			$data = $line;
			chomp $data;
			last;
		}
		$linenum++;
	}
	close $fh;
	return $data;
}

sub count_input_records
{
	# TODO use index file, if exists.
	
	my $fh = fopen $InputFile, {no_create=>1,};
	my $linenum = 0;
	$linenum++ while scalar <$fh>;
	close $fh;
	$TotalDataNum = $linenum;
	return $linenum;
}

sub processing_lockfile_name
{
	my $processing_number = shift;
	return "$InputFile.uniproc.$processing_number";
}

sub still_in_progress
{
	my $processing_number = shift;
	my $lockfile = processing_lockfile_name($processing_number);
	open my $fh, '<', $lockfile or return 0;
	my $lock_ok = flock $fh, LOCK_EX|LOCK_NB;
	close $fh;
	return not $lock_ok;
}

sub get_next_data_number
{
	debug_msg "go to offset ".($FirstUnprocessedRecord*$ResultsRecordLength)." to read status of record # $FirstUnprocessedRecord\n";
	seek $Results_fh, $FirstUnprocessedRecord*$ResultsRecordLength, SEEK_SET;
	
	my $record_num = $FirstUnprocessedRecord;
	my $result;
	while(1)
	{
		my $nbytes = read $Results_fh, $result, $ResultsRecordLength;
		debug_msg "read only $nbytes bytes \"$result\" at record $record_num\n" if $nbytes < $ResultsRecordLength;
		last if $nbytes < $ResultsRecordLength;
		chomp $result;
		
		if($result eq '   ' and $OptProcessNew)
		{
			$FirstUnprocessedRecord = $record_num+1;
			debug_msg "uninitialized $record_num\n";
			return $record_num;
		}
		if($result eq '...' and $OptProcessNew)
		{
			# check if still locked
			if(not still_in_progress($record_num))
			{
				$FirstUnprocessedRecord = $record_num+1;
				debug_msg "crashed $record_num\n";
				return $record_num;
			}
		}
		if($OptProcessFailed and ($result =~ /^!/ or ($result =~ /^\s*\d+$/ and $result > 0)))
		{
			$FirstUnprocessedRecord = $record_num+1;
			debug_msg "retry $record_num\n";
			return $record_num;
		}
		
		$record_num++;
	}
	
	return undef if not $OptProcessNew;
	
	# check here if there are more input data than result records
	my $input_records = count_input_records();
	if($record_num < $input_records)
	{
		extend_resultsfile($Results_fh, $input_records * $ResultsRecordLength);
		$FirstUnprocessedRecord = $record_num+1;
		debug_msg "new $record_num\n";
		return $record_num;
	}
	
	# no more input data
	debug_msg "no more input. input_records=$input_records\n";
	return undef;
}

sub subst_data
{
	my $data = shift;
	my $field_num = shift;
	my $fields = shift;
	if($field_num =~ /^\d+$/)
	{
		return $fields->[$field_num-1];
	}
	return $data;
}

sub expand_placeholders
{
	my $arg = shift;
	my $Data = shift;
	my $DataFields_ref = shift;
	my $placeholder_found_ref = shift;
	
	if($arg =~ s/\{(\d*)\}/subst_data($Data, $1, $DataFields_ref)/eg)
	{
		$$placeholder_found_ref = 1;
		return $arg;
	}
	if(my ($indices) = $arg =~ /^\{([\d,]+|\@)\}$/)
	{
		$$placeholder_found_ref = 1;
		return @$DataFields_ref if $indices eq '@';
		return map {$DataFields_ref->[$_-1] // ''} split /,/, $indices;
	}
	return $arg;
}


($InputFile, @CommandArgs) = @ARGV;

$FirstUnprocessedRecord = 0;
$ResultsRecordLength = 4;
$ResultsFile = "$InputFile.uniproc";
$Results_fh = undef;
$num_shots = 0;
$TotalDataNum = undef;
$ExitStatus = 0;

while(not defined $OptShots or $num_shots < $OptShots)
{
	$Results_fh = fopen $ResultsFile, {rw=>1, lock=>1};
	my $LineNum = get_next_data_number();
	last if not defined $LineNum;
	count_input_records if not defined $TotalDataNum;
	
	my $Data = input_data($LineNum);
	my @DataFields = split /\t/, $Data;
	
	my $InprogressFile = processing_lockfile_name($LineNum);
	my $inprogress_fh = fopen $InprogressFile, {rw=>1, lock=>1};
	
	record_status($LineNum, '...');
	close $Results_fh;
	
	printf STDERR "uniproc: %d/%d%s%s\n", $LineNum+1, $TotalDataNum, ($OptShowData ? (": ", $Data) : ("", "")) if $OptShowProgress;
	
	$ENV{'UNIPROC_DATANUM'} = $LineNum;
	$ENV{'UNIPROC_DATANUM_1INDEX'} = $ENV{'UNIPROC_DATANUM'}+1;
	$ENV{'UNIPROC_TOTALNUM'} = $TotalDataNum;
	
	my $placeholder_found = 0;
	my @command_args = map {expand_placeholders($_, $Data, \@DataFields, \$placeholder_found)} @CommandArgs;
	if(not $placeholder_found)
	{
		push @command_args, @DataFields;
	}
	
	my $stdin = '';
	run [@command_args], \$stdin;
	my $status = $?;
	
	$Results_fh = fopen $ResultsFile, {rw=>1, lock=>1};
	record_status($LineNum, deriv_processing_status($status));
	close $Results_fh;
	
	if(WIFSIGNALED($status) or WEXITSTATUS($status)) { $CountFailed++; }
	else { $CountSuccess++; }
	
	unlink $InprogressFile;
	close $inprogress_fh;
	
	if($OptErrExit)
	{
		$ExitStatus = WIFSIGNALED($status) ? 128 + WTERMSIG($status) : WEXITSTATUS($status);
		last;
	}
	
	$num_shots++;
}

if($OptShowSummary)
{
	printf STDERR "uniproc: %d total, %d processed in this session, %d successful, %d failed\n", $TotalDataNum, $CountSuccess+$CountFailed, $CountSuccess, $CountFailed;
}

exit $ExitStatus;
